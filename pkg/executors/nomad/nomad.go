package nomad

import (
	"dag/hector/golang/module/pkg/definitions"
	"dag/hector/golang/module/pkg/jobs"
	"dag/hector/golang/module/pkg/results"
	"fmt"
	"time"

	"github.com/hashicorp/nomad/api"
	"golang.org/x/exp/slices"
)

func argumentsToSlice(argumentsPointer *[]definitions.Parameter) []string {
	var args []string
	for _, arg := range *argumentsPointer {
		args = append(args, "--"+arg.Name)
		args = append(args, arg.Value.(string))
	}
	return args
}

type Nomad struct {
	Client *api.Client
}

func NewNomad() *Nomad {
	cfg := api.DefaultConfig()
	client, _ := api.NewClient(cfg)
	return &Nomad{Client: client}
}

func (no *Nomad) ExecuteJob(jobPointer *jobs.Job) (*results.ResultJob, error) {

	// We print the initialization message and display the job information
	fmt.Printf("Started "+(*jobPointer).Name+" job. Info: \n\t %+v\n\n", *jobPointer)

	// We build the nomad job with the information stored in the input pointer
	// 1. Task
	taskName := "Task-" + (*jobPointer).Id
	args := argumentsToSlice(&(*jobPointer).Arguments)
	attempts := 0
	nomadTaskPointer := &api.Task{
		Name:   taskName,
		Driver: "docker",
		Config: map[string]interface{}{
			"image": (*jobPointer).Image,
			"args":  args,
		},
		RestartPolicy: &api.RestartPolicy{Attempts: &attempts},
	}

	// 2. Task Group
	taskGroupName := "Task-Group-" + (*jobPointer).Id
	nomadTaskGroupPointer := &api.TaskGroup{
		Name:          &taskGroupName,
		Tasks:         []*api.Task{nomadTaskPointer},
		RestartPolicy: &api.RestartPolicy{Attempts: &attempts},
	}

	// 3. Job
	jobType := "batch"
	nomadJobPointer := &api.Job{
		ID:          &(*jobPointer).Id,
		Name:        &(*jobPointer).Name,
		Type:        &jobType,
		Datacenters: []string{"dc1"},
		TaskGroups:  []*api.TaskGroup{nomadTaskGroupPointer},
		Reschedule:  &api.ReschedulePolicy{Attempts: &attempts},
	}

	// We create the variable logs to store all the information associated with the definition of the job
	var logs string

	// We start to execute the job
	jobRegisterResponsePointer, _, err := no.Client.Jobs().Register(nomadJobPointer, nil)
	if err != nil {
		return nil, err
	}

	// Delete job and clean up system after function execution
	defer no.Client.Jobs().Deregister((*jobPointer).Id, true, nil)
	defer no.Client.System().GarbageCollect()

	// If there are any warnings, they are stored in logs
	if warnings := (*jobRegisterResponsePointer).Warnings; warnings != "" {
		logs += warnings + "\n"
	}

	// We wait for the execution to finish
	// NOTE: https://github.com/hashicorp/nomad/issues/6818
	status := results.Waiting
	for status == results.Waiting {

		// We establish pauses of 10 milliseconds
		time.Sleep(10 * time.Millisecond)

		// We obtain the most summarized information of our job (minimum amount of information found so as not to overload the loop)
		jobSummary, _, err := no.Client.Jobs().Summary((*jobPointer).Id, nil)
		if err != nil {
			return nil, err
		}

		// If our task group has finished, we change status
		if jobSummary.Summary[taskGroupName].Complete == 1 {
			status = results.Done
		} else if jobSummary.Summary[taskGroupName].Failed == 1 {
			status = results.Error
		}
	}

	// We print the finalization message
	fmt.Println("Finished " + (*jobPointer).Name + " job\n")

	// Get the list of allocations generated by our job
	allocs, _, err := no.Client.Jobs().Allocations((*jobPointer).Id, true, nil)
	if err != nil {
		return nil, err
	}

	// Since the constructed job has only one task group, we limit the number of allocations to 1
	if len(allocs) != 1 {
		return nil, fmt.Errorf("unexpected number of allocs: %d", len(allocs))
	}

	// We obtain the information of the allocation found
	alloc, _, err := no.Client.Allocations().Info(allocs[0].ID, nil)
	if err != nil {
		return nil, err
	}

	// If the status of the task is Error then we look for errors caused by docker in loading the image. In that case we return the result job without scanning the task logs.
	if status == results.Error {
		idxFailure := slices.IndexFunc(alloc.TaskStates[taskName].Events, func(eventPointer *api.TaskEvent) bool { return (*eventPointer).Type == "Driver Failure" })
		if idxFailure != -1 {
			return &results.ResultJob{Id: (*jobPointer).Id, Name: (*jobPointer).Name, Logs: alloc.TaskStates[taskName].Events[idxFailure].DisplayMessage, Status: status}, nil
		}
	}

	// Select the reading channel according to the task status
	var channel string
	if status == results.Done {
		channel = "stdout"
	} else {
		channel = "stderr"
	}

	// Get logs from our allocation
	cancel := make(chan struct{})
	frames, errors := no.Client.AllocFS().Logs(alloc, false, taskName, channel, "start", 0, cancel, nil)

	// Extract information from channels
	select {

	// We will notify any error
	case logErr := <-errors:
		if logErr != nil {
			return nil, logErr
		}

	// Extract the contents of the logs
	case frame := <-frames:
		logs += string(frame.Data)
	}

	// We return the result job
	return &results.ResultJob{Id: (*jobPointer).Id, Name: (*jobPointer).Name, Logs: logs, Status: status}, nil
}
